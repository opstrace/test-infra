# Manually increase some ingester limits, otherwise we will see HTTP errors, paired with error messages in cortex pods.
# NOTE 1: May need to manually "kubectl delete pod -n kube-system opstrace-controller-..." for the change to kick in
# NOTE 2: It can take 20mins to 1hr for the ingester statefulset to roll out the change
apiVersion: v1
kind: ConfigMap
metadata:
  name: opstrace-controller-config-overrides
  namespace: default
data:
  # Overrides to the cortex/cortex-config configmap in the opstrace cluster
  # Merged result is written to /etc/cortex/config.yaml
  cortex: |
    limits:
      # These two series limits VERY QUICKLY result in HTTP 400 errors paired with this ingester error:
      #  per-user series limit of 5000000 exceeded, please contact administrator to raise it (local limit: 5000000 global limit: 10000000 actual local limit: 3333333)
      # Here we increase the limits by 100x from the default controller values
      max_global_series_per_user: 1000000000 # controller: 10000000
      max_series_per_user: 500000000 # controller: 5000000
      # If we go even further, we start seeing another bottleneck: HTTP 503 errors paired with this distributor error:
      #  ingestion rate limit (100000) exceeded while adding 2000 samples and 0 metadata
      # Here we increase the limit by 100x from the default controller value
      ingestion_rate: 100000000 # controller: 100000, default: 25000
    # NOTE: DO NOT SET INGESTER LIMITS HERE.
    # They will be ignored in favor of the runtime configuration (even if that configuration is empty).
    # Add any ingester limits to cortex-runtime-config below.

---

# Runtime configuration required to set some limits.
# There's code to set it via the Opstrace app/UI but we just short-circuit and set it directly via kubectl.
# Schema defined here: https://github.com/cortexproject/cortex/blob/master/pkg/cortex/runtime_config.go#L24
# Incomplete example here: https://cortexmetrics.io/docs/configuration/arguments/#runtime-configuration-file
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
    opstrace: no-update # mark as immutable so controller doesn't overwrite
  name: cortex-runtime-config
  namespace: cortex
data:
  # Runtime configuration, some settings in the main config are ignored when this is enabled.
  # Written to /etc/cortex-runtime-cfg/runtime-config.yaml
  runtime-config.yaml: |
    ingester_limits:
      # Set a cap on how many push requests can be in-flight at a time.
      # Without this set, one ingester at a time was observed to OOM as pending requests stacked up.
      # On r5.16xlarge nodes, OOMing was observed on as few as 121k pending requests, so we go with 20k for now to play it safe.
      # The issue is typically only for one ingester at a time, despite sharding theoretically spreading the work across ingesters evenly.
      # Once this is set the effective throughput ceiling seems to greatly increase, so maybe setting a limit helps performance?
      # See also system tenant metric:
      #    sum(cortex_inflight_requests{route="/cortex.Ingester/Push",service="ingester"}) by (pod)
      # Can validate that this setting is actually working by checking config metrics:
      #    max(cortex_ingester_instance_limits{limit="max_inflight_push_requests"}) by (pod)
      max_inflight_push_requests: 20000 # controller: unset, default: 0/unlimited
