#
# Step 1:  set up access to the resources we need.
#
# We must create a service account and bind it to a role that has
# sufficient permissiones to collect the information we need.  I
# The following permissions are correct, but they may not be the
# smallest possible set.
#
# NOTE:  For the workload clusters, we just use the default namespace.
#
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: agent
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: agent
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
      - namespaces
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: agent
subjects:
  - kind: ServiceAccount
    name: agent
    namespace: default
#
# Step 2:  set up the configuration of our collectors.
#
# Here we use ConfigMaps to store the configuration for our collectors.
# We will map these to specific files inside the container, from which
# the collectors can read them.
#
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
data:
  promtail.yaml: |
    # TODO: switch to using pipeline stages properly.
    # https://github.com/grafana/loki/blob/master/docs/clients/promtail/pipelines.md
    client:
      url: https://loki-external.system.%NAME%.gcp.opstrace.io:8443/loki/api/v1/push
      tls_config:
        insecure_skip_verify: true
    server:
      log_level: debug
    scrape_configs:
      - job_name: kubernetes-pods-name
        pipeline_stages:
          - docker: {}
        
        # Use Kubernetes Service Discovery to find taret pods.
        # 
        # The "pod" role provides the following available metadata:
        # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#pod
        # 
        # Note that while Promtail can utilize the Kubernetes API to 
        # discover pods as targets, it can only read log files from pods
        # that are running on the same node as the one Promtail is running
        # on. Promtail looks for a __host__ label on each target and 
        # validates that it is set to the same hostname as Promtail's 
        # (using either $HOSTNAME or the hostname reported by the kernel if
        # the environment variable is not set).
        # https://github.com/grafana/loki/blob/master/docs/clients/promtail/scraping.md#kubernetes-discovery
        #
        # TODO(fix): The Prometheus DaemonSet is not being collected. 
        #
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
        - source_labels:
            - __meta_kubernetes_pod_node_name
          target_label: __host__
        - source_labels:
          - __meta_kubernetes_pod_label_name
          target_label: __service__
        - action: drop
          regex: ^$
          source_labels:
            - __service__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - action: replace
          replacement: workload_$1
          source_labels:
            - __service__
          target_label: job
        - action: replace
          source_labels:
            - __meta_kubernetes_namespace
          target_label: namespace
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_name
          target_label: pod_name
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_container_name
          target_label: container_name
        - action: replace
          replacement: %NAME%
          target_label: cluster_name

        # Define the path to the container files available on the host. We
        # must then mount these into the container as volumes.
        - source_labels: 
            - __meta_kubernetes_pod_uid
            - __meta_kubernetes_pod_container_name
          target_label: __path__
          separator: /
          replacement: /var/log/pods/*$1/*.log
---
apiVersion: v1
data:
  prometheus.yml: |
    remote_write:
      - basic_auth:
        url: "https://prometheus-external.default.%NAME%.gcp.opstrace.io:8443/api/prom/push"
        tls_config:
          insecure_skip_verify: true
    scrape_configs:
      - job_name: "avalanche"
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: keep
            regex: "true"
            source_labels: 
              - __meta_kubernetes_pod_annotation_avalanche_scrape
      - job_name: "looker"
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: keep
            regex: "true"
            source_labels:
              - __meta_kubernetes_pod_annotation_looker_scrape
        tls_config:
          insecure_skip_verify: true
      - job_name: "promtail"
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: keep
            regex: "true"
            source_labels:
              - __meta_kubernetes_pod_annotation_promtail_scrape
        tls_config:
          insecure_skip_verify: true
      - job_name: "workload_prometheus"
        scrape_interval: 10s
        static_configs:
          - targets: ['localhost:9090']
        tls_config:
          insecure_skip_verify: true
    global:
      scrape_interval: 30s
kind: ConfigMap
metadata:
  name: prometheus-config
#
# Step 3:  Configure DaemonSets for the collectors.
#
# We want at most one collector per workload node.  The semantics of a
# DaemonSet are a natural fit for this.  (Rather than using a Deployment
# with anti-affinity rules.)
#
# These DaemonSets should use the agents from Step 1.  The must mount in
# the ConfigMaps from Step 2 as volumes, as well as any other volumes that
# are necessary (see Promtail).
#
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
spec:
  selector:
    matchLabels:
      name: promtail
  template:
    metadata:
      labels:
        name: promtail
    spec:
      serviceAccount: agent
      serviceAccountName: agent
      volumes:
        - name: pods
          hostPath:
            path: /var/log/pods
        - name: logs-docker
          hostPath:
            path: /var/lib/docker/containers
        - name: promtail-config
          configMap:
            name: promtail-config
      containers:
        - name: promtail-container
          image: grafana/promtail # TODO: select a specific version
          imagePullPolicy: IfNotPresent

          # We must pass the hostname into the container's environment, as
          # the "OS" was returning the pod name instead of the underlying
          # node name.
          env:
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          args:
            - -config.file=/etc/promtail/promtail.yaml # mounted from the ConfigMap
          securityContext:
            privileged: true # required to access the container log files
          volumeMounts:
            - name: pods
              mountPath: /var/log/pods
            - name: logs-docker
              mountPath: /var/lib/docker/containers
            - name: promtail-config
              mountPath: /etc/promtail
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: prometheus
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      name: prometheus
  template:
    metadata:
      labels:
        name: prometheus
    spec:
      # We are currently using a single Prometheus instance to collect from
      # all of the Avalanche containers.  This requires sigificant resources
      # and therefore we must run it on a unique instance type.  Here, we
      # definie a special node pool just for Prometheus.  This node pool
      # should be several times larger than the workload instances themselves.
      #
      # TODO:  Switch to the Grafana Cloud Agent, and run on every hosts like
      # we do with Promtail.  See [ch53].
      nodeSelector:
        cloud.google.com/gke-nodepool: prom-pool
      containers:
        - args:
            - --config.file=/etc/prom/prometheus.yml
            - --log.level=debug
          env:
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          image: prom/prometheus # TODO: select a specific version
          imagePullPolicy: IfNotPresent
          name: prometheus
          ports:
            - containerPort: 80
              name: http-metrics
          securityContext:
            privileged: true # TODO: is this really needed for Prom?
            runAsUser: 0
          volumeMounts:
            - mountPath: /etc/prom
              name: prometheus-config
      serviceAccount: agent
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - configMap:
            name: prometheus-config
          name: prometheus-config
      # - job_name: kubernetes-pods-app
      #   pipeline_stages:
      #     - docker: {}
      #   kubernetes_sd_configs:
      #     - role: pod
      #   relabel_configs:
      #     - action: drop
      #       regex: .+
      #       source_labels:
      #       - __meta_kubernetes_pod_label_name
      #     - source_labels:
      #       - __meta_kubernetes_pod_label_app
      #       target_label: __service__
      #     - source_labels:
      #       - __meta_kubernetes_pod_node_name
      #       target_label: __host__
      #     - action: drop
      #       regex: ^$
      #       source_labels:
      #       - __service__
      #     - action: labelmap
      #       regex: __meta_kubernetes_pod_label_(.+)
      #     - action: replace
      #       replacement: $1
      #       separator: /
      #       source_labels:
      #       - __meta_kubernetes_namespace
      #       - __service__
      #       target_label: job
      #     - action: replace
      #       source_labels:
      #       - __meta_kubernetes_namespace
      #       target_label: namespace
      #     - action: replace
      #       source_labels:
      #       - __meta_kubernetes_pod_name
      #       target_label: instance
      #     - action: replace
      #       source_labels:
      #       - __meta_kubernetes_pod_container_name
      #       target_label: container_name
      #     - replacement: /var/log/pods/*$1/*.log
      #       separator: /
      #       source_labels:
      #       - __meta_kubernetes_pod_uid
      #       - __meta_kubernetes_pod_container_name
      # #       target_label: __path__
      # -  job_name: kubernetes-pods
      #    kubernetes_sd_configs:
      #      - role: pod
      #    relabel_configs:
      #      - source_labels: ['__meta_kubernetes_pod_node_name']
      #        target_label: '__host__'
      #     #  - action: keep
      #     #    regex: "true"
      #     #    source_labels:
      #     #      - __meta_kubernetes_pod_annotation_looker_scrape
      #      - action: replace
      #        source_labels:
      #          - __meta_kubernetes_namespace
      #        target_label: namespace
      #      - action: labelmap
      #        regex: __meta_kubernetes_pod_label_(.+)
      #        - action: drop
      #          regex: "false"
      #          source_labels:
      #            - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      #    - action: keep
      #      regex: .*-metrics
      #      source_labels:
      #        - __meta_kubernetes_pod_container_port_name
      #    - action: replace
      #      regex: (https?)
      #        replacement: $1
      #        source_labels:
      #          - __meta_kubernetes_pod_annotation_prometheus_io_scheme
      #        target_label: __scheme__
      #      - action: replace
      #        regex: (.+)
      #        replacement: $1
      #        source_labels:
      #          - __meta_kubernetes_pod_annotation_prometheus_io_path
      #        target_label: __metrics_path__
      #      - action: replace
      #        regex: (.+?)(\:\d+)?;(\d+)
      #        replacement: $1:$3
      #        source_labels:
      #          - __address__
      #          - __meta_kubernetes_pod_annotation_prometheus_io_port
      #        target_label: __address__
      #      - action: drop
      #        regex: ""
      #        source_labels:
      #          - __meta_kubernetes_pod_label_name
      #      - action: replace
      #        replacement: $1
      #        separator: /
      #        source_labels:
      #          - __meta_kubernetes_namespace
      #          - __meta_kubernetes_pod_label_name
      #        target_label: job
      #      - action: replace
      #        source_labels:
      #          - __meta_kubernetes_namespace
      #        target_label: namespace
      #      - action: replace
      #        source_labels:
      #          - __meta_kubernetes_pod_name
      #        target_label: instance
      #      - action: labelmap
      #        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
      #        replacement: __param_$1
      #      - action: drop
      #        regex: Succeeded|Failed
      #        source_labels:
      #          - __meta_kubernetes_pod_phase
