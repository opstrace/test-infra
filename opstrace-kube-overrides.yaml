# Manually increase some ingester limits, otherwise we will see HTTP errors, paired with error messages in cortex pods.
# NOTE 1: May need to manually "kubectl delete pod -n kube-system opstrace-controller-..." for the change to kick in
# NOTE 2: It can take 20mins to 1hr for the ingester statefulset to roll out the change
apiVersion: v1
kind: ConfigMap
metadata:
  name: opstrace-controller-config-overrides
  namespace: default
data:
  # Overrides to the cortex/cortex-config configmap in the opstrace cluster
  # Merged result is written to /etc/cortex/config.yaml
  cortex: |
    limits:
      # These two series limits VERY QUICKLY result in HTTP 400 errors paired with this ingester error:
      #  per-user series limit of 5000000 exceeded, please contact administrator to raise it (local limit: 5000000 global limit: 10000000 actual local limit: 3333333)
      # Here we increase the limits by 100x from the default controller values
      max_global_series_per_user: 1000000000 # controller: 10000000
      max_series_per_user: 500000000 # controller: 5000000

      # If we go even further, we start seeing another bottleneck: HTTP 503 errors paired with this distributor error:
      #  ingestion rate limit (100000) exceeded while adding 2000 samples and 0 metadata
      # Here we increase the limit by 100x from the default controller value
      ingestion_rate: 100000000 # controller: 100000, default: 25000

      # With higher cardinality test data (avalanche: --series-count), we also need to increase this limit.
      max_series_per_metric: 50000000 # default: 50000

      # When testing metrics with LOTS of labels, we need to increase this limit.
      max_label_names_per_series: 100000 # default: 30 (testing up to 10k, but leaving plenty of headroom)
    ingester_client:
      # When testing metrics with LOTS of labels, we also need to increase this limit.
      grpc_client_config:
        max_send_msg_size: 1048576001 # default: 16777216
        grpc_compression: gzip # default: ""
    distributor:
      # At 10k label metrics, this starts to become a bottleneck (e.g. seeing 218MB payloads).
      max_recv_msg_size: 1048576002 # default: 104857600
      # Avoid risk of OOMing if the ingesters are falling behind or down
      instance_limits:
        max_inflight_push_requests: 10000
    server:
      # At 10k label metrics, these (barely) start to become a bottleneck (e.g. seeing 46MB payloads).
      grpc_server_max_recv_msg_size: 419430401 # default: 41943040
      grpc_server_max_send_msg_size: 419430402 # default: 41943040
    # NOTE: DO NOT SET INGESTER RUNTIME LIMITS HERE.
    # They will be ignored in favor of the runtime configuration (even if that configuration is empty).
    # Add any ingester limits to cortex-runtime-config.

---

# Runtime configuration required to set some limits.
# There's code to set it via the Opstrace app/UI but we just short-circuit and set it directly via kubectl.
# Schema defined here: https://github.com/cortexproject/cortex/blob/master/pkg/cortex/runtime_config.go#L24
# Incomplete example here: https://cortexmetrics.io/docs/configuration/arguments/#runtime-configuration-file
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
    opstrace: no-update # mark as immutable so controller doesn't overwrite
  name: cortex-runtime-config
  namespace: cortex
data:
  # Runtime configuration, some settings in the main config are ignored when this is enabled.
  # Written to /etc/cortex-runtime-cfg/runtime-config.yaml
  runtime-config.yaml: |
    ingester_limits:
      # Set a cap on how many push requests can be in-flight at a time.
      # Without this set, one ingester at a time was observed to OOM as pending requests stacked up.
      # On r5.16xlarge nodes, OOMing was observed on as few as 121k pending requests, so we go with 20k for now to play it safe.
      # After OOMing, the ingester takes a long time to come back, since it needs to recover its WAL,
      # resulting in a potential loop of ingesters going down until incoming load has been decreased.
      # The issue is typically only for one ingester at a time, despite sharding theoretically spreading the work across ingesters evenly.
      # See also system tenant metric:
      #    sum(cortex_inflight_requests{route="/cortex.Ingester/Push",service="ingester"}) by (pod)
      # Can validate that this setting is actually working by checking config metrics:
      #    max(cortex_ingester_instance_limits{limit="max_inflight_push_requests"}) by (pod)
      max_inflight_push_requests: 10000 # controller: unset, default: 0/unlimited
